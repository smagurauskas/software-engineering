{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```mermaid\n",
    "mindmap\n",
    "  root((Quality assurance))\n",
    "    (Testing)\n",
    "      Manual testing\n",
    "      Automated testing\n",
    "        Unit testing\n",
    "        Integration testing\n",
    "        E2E testing\n",
    "        ...\n",
    "    (Coding standards)\n",
    "      Static analysis\n",
    "      Linting rules\n",
    "      Style guidelines\n",
    "    (Code reviews)\n",
    "      Pull requests\n",
    "    (Other activities)\n",
    "      ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Testing is a part of overall software quality assurance process. Testing is used to ensure that the developed software does what it is supposed to do by trying it out.\n",
    "\n",
    "Software engineers usually does testing while automated test development. Automated tests take on many forms depending on the scope of what is being tested.\n",
    "\n",
    "Automated test's scope can range from testing a single method by passing predefined inputs and expecting predefined outputs, to running a whole application, including dependencies like database, and testing that the application behaves correctly when interacted via UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The most simple automated test is just a dedicated method that calls another method with predefined input, and asserts that returned results is what was expected.\n",
    "\n",
    "```csharp\n",
    "public int AddNumber(int a, int b)\n",
    "{\n",
    "    return a + b;\n",
    "}\n",
    "\n",
    "public void Test_AddNumber()\n",
    "{\n",
    "    if (AddNumber(5, 9) == 14)\n",
    "    {\n",
    "        Console.WriteLine(\"Test passed\");\n",
    "        return;\n",
    "    }\n",
    "\n",
    "    Console.WriteLine(\"Test_AddNumber failed\");\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Usually the method you are testing are going to have more functionality and they will have to be tested against several cases:\n",
    "\n",
    "```csharp\n",
    "public double PerformMath(string expression)\n",
    "{\n",
    "    var parts = expression.Split(' ');\n",
    "    var a = double.Parse(parts[0]);\n",
    "    var b = double.Parse(parts[2]);\n",
    "\n",
    "    switch (parts[1])\n",
    "    {\n",
    "        case \"+\":\n",
    "            return a + b;\n",
    "        case \"-\":\n",
    "            return a - b;\n",
    "        case \"*\":\n",
    "            return a * b;\n",
    "        case \"/\":\n",
    "            return a / b;\n",
    "        default:\n",
    "            throw new ArgumentException(\"Invalid operator\");\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "Test methods:\n",
    "\n",
    "```csharp\n",
    "public void PerformMath_Addition_Works()\n",
    "{\n",
    "    var result = PerformMath(\"2 + 3\");\n",
    "    \n",
    "    if (result != 5)\n",
    "    {\n",
    "        throw new Exception(\"Addition failed\");\n",
    "    }\n",
    "\n",
    "    Console.WriteLine(\"Addition works\");\n",
    "}\n",
    "\n",
    "public void PerformMath_Subtraction_Works()\n",
    "{\n",
    "    var result = PerformMath(\"2 - 3\");\n",
    "    \n",
    "    if (result != -1)\n",
    "    {\n",
    "        throw new Exception(\"Subtraction failed\");\n",
    "    }\n",
    "\n",
    "    Console.WriteLine(\"Subtraction works\");\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What the automated tests should be like\n",
    "\n",
    "Good automated tests are:\n",
    "- Fast to execute.\n",
    "- Predictable, meaning that there are no variations between runs.\n",
    "- Easy to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Fast to run tests\n",
    "\n",
    "Test should run relatively fast. The longer the tests are going to run, the less frequently engineers are going to run them. For a medium sized project ~1min to run the tests is reasonable. If the tests were to run longer, for let's say 10 minutes, then it is almost guaranteed that engineers would run them very rarely, maybe once before the pull request.\n",
    "\n",
    "The less frequently the tests are ran, the less obvious benefit they bring, so the less intrinsic motivation engineers have to maintain and expand them.\n",
    "\n",
    "Same logic applies to launching the tests. Tests should be launched with a single command. No additional commands, no additional service startups or environment preparations must be needed. The more setup tests requires, the less likely they are going to be ran because of the friction. Good tests are started with a single command and no external dependencies.\n",
    "\n",
    "However there are some possible trade-offs when the longer running test might be tolerable if thoroughly tests a large project or many smaller services."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Measuring test coverage  \n",
    "Test coverage percentage is measured to determine how much of the code is executed during testing. It is used to evaluate whether tests cover critical parts of the application and to identify areas that remain untested.  \n",
    "\n",
    "Coverage is calculated based on metrics collected while tests are executed. Line coverage counts the percentage of lines that were run, and branch coverage measures whether all branches in conditional statements were tested. These values are aggregated to provide a total percentage that reflects how much of the code was covered.  \n",
    "\n",
    "In .NET, test coverage can be measured using Coverlet. Reports are generated automatically, showing the percentage of covered code and marking tested and untested areas.  \n",
    "\n",
    "Some parts of the code should be excluded from coverage calculation. This includes generated code, third-party libraries, trivial code like simple getters and setters, utility functions, and code related to debugging or logging. Excluding these parts gives a more accurate view of meaningful test coverage.  \n",
    "\n",
    "While test coverage percentage is useful for identifying untested areas, it is not a very reliable metric for software quality. High coverage may show that code is executed, but it does not ensure that behavior is tested correctly. However, it is quick to implement and can be helpful when combined with other practices like code reviews to improve software reliability.  "
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Test predictability\n",
    "\n",
    "Tests must be predictable between runs. This means that if the code under testing was not changed and the test itself was not changed, then the outcome of the test must be always the same, regardless of how many times or when it was ran.\n",
    "\n",
    "Tests that are not predictable between runs are called *brittle*.\n",
    "\n",
    "Tests become brittle if they are using some shared state or relying on some uncontrolled, global, variables.\n",
    "\n",
    "Examples of shared state include using a live database, shared sqlite database file or a shared real implementation of some API. This means that the tests might influence each others data and the subsequent tests can run against the data leftovers of the previous test\n",
    "\n",
    "Examples of global variables could be `DateTime.UtcNow` or similar date constructs. Because they are `static` it is difficult to mock them. On top of that they are not controllable by user, but are rather provided by the runtime. If the tests assumes that some date is always going to be future date and asserts it against some predefined date. Eventually the future will arrive and the date will become past date and the test will begin to fail. "
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Types of tests\n",
    "\n",
    "```mermaid\n",
    "mindmap\n",
    "  root((Quality assurance))\n",
    "    (Automated testing)\n",
    "      Unit testing\n",
    "      Integration testing\n",
    "      E2E testing\n",
    "      ...\n",
    "```"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Unit tests\n",
    "\n",
    "Unit tests are testing a single unit. The definition of unit can vary a lot, it can range from single method, to the whole assembly.\n",
    "\n",
    "Whatever the size of unit is decided to be, such tests, by definition, needs to test that unit in isolation. Testing in isolation means, that all dependencies of that unit are substituted with fakes or mocks, and only the logic of that unit is tested.\n",
    "\n",
    "The upside of this is that the tests allows to isolate the source of the problem easier, because an exact failing unit is known. However, the downside is that this warrants much more tests to cover all of the needed functionality, because these tests end up being small in their scope."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Integration tests\n",
    "\n",
    "Integrations tests several units at once. You could say that integration tests test how well the unit integrate with each other.\n",
    "\n",
    "In practice this means that integration test can several modules at once, or test an application together with database or with some other dependency.\n",
    "\n",
    "However it is still crucial that the dependencies used would work deterministically, which means that testing with some development or production database is not the best idea. Running a special containerized instance of the database in such case would be a better solution.\n",
    "\n",
    "In ASP.NET integration tests could be done by using `TestServer` which allows to override startup procedures of generic host applications and launch them in custom mode. THis would allow to test partially assembled application, which depending on the architecture, could be convenient."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### E2E tests\n",
    "\n",
    "E2E stands for end-to-end. E2E tests test all the application by running full production like environment and then interacting with the application via the UI.\n",
    "\n",
    "The upsides of such tests is that they reflect the way users will be using the application the best. In the end it doesn't matter if the `Calculator` class is implemented perfectly, if the button that is supposed to show the result does not work.\n",
    "\n",
    "Another upside is that a single test can test multiple layers and use cases of the application, so relatively little amount of such tests produce large coverage.\n",
    "\n",
    "The downside of E2E tests is that they tend to be brittle. For example changing the button label might make such tests to fail. Other downside is that such tests are not very specific in pointing engineering to the root cause of what is causing the test to fail. They can tell you that something is not working, but not exactly what."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Alternative categorization of automated tests\n",
    "\n",
    "```mermaid\n",
    "mindmap\n",
    "  root(Quality assurance)\n",
    "    Automated testing\n",
    "      Small tests\n",
    "      Medium tests\n",
    "      Large tests\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why automated tests?\n",
    "\n",
    "```mermaid\n",
    "mindmap\n",
    "  root(Automated testing)\n",
    "    Goals of the automated testing\n",
    "      Increased code quality\n",
    "      Increased trust in the application\n",
    "      Increased development velocity\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Automated tested is supposed to help quickly and repeatedly reliably determine if the code is functioning as expected. This in turn should help to increase the development velocity over time. Even if it does not look worthwhile to write tests at the beginning, because writing the tests themselves takes time, it is largely accepted that writing them pays off over sufficiently long time.\n",
    "\n",
    "The term \"*sufficiently long time*\" might be a bit ambiguous, so to put more concreteness into it: if you are guaranteed that some script you are writing is only going to be ran once and only by you, then it might not be worthwhile to write automated tests as testing manually might indeed be faster. However if it is some project that you will be working on for a month, it is most likely already worthwhile to write some automated tests for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Automated tests should tell you:\n",
    "1. Is the code being tested working as expected?\n",
    "2. If it is not working, then point you in the direction of problems.\n",
    "\n",
    "The 1st point is obvious, while the 2nd point has much more to it than it might appear at first. Firstly the failing test should indicate what part of code is not working so you would know what you should investigate. This can be done via test naming, custom failure exception messages or combination of both. Usually it is easier to do this via test names.\n",
    "\n",
    "Secondly, even if by the failing test name you know the use case that is failing, it would still be nice if the test could point you even more. This can be achieved via very clean test method code. The test code must be very legible. If the test code is very clear, then it should be very easy to trace how the assertion value that failed was acquired. Knowing the exact sequence of how the failure was produced should help you finding the source of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tests cannot ensure 100% quality\n",
    "\n",
    "![Test coverage vs cost](assets/test_coverage_vs_cost.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Tests can only ensure that only what was tested works. It is increasingly difficult to cover 100% of application in tests. This difficulty makes it economically impractical to strive for such coverage in most cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "There are always going to be additional freaky argument pair that was not tested. Just because `\"2 + 3\"` argument worked in the last example, you cannot be 100% guaranteed that `\"2 + 4\"` will work.\n",
    "\n",
    "However you can decide to be confident that if `\"2 + 4\"`, `\"2 + 4\"`, and `\"4 + 3\"` works, then maybe all other addition operation are going to work as well. You need to decide when you are confident with the existing tests enough and it is no longer worth the effort writing more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Frameworks\n",
    "\n",
    "```mermaid\n",
    "mindmap\n",
    "  root((Testing frameworks))\n",
    "    Python\n",
    "      PyTest\n",
    "      Unittest\n",
    "      Nose2\n",
    "    JavaScript\n",
    "      Jest\n",
    "      Mocha\n",
    "      Jasmine\n",
    "    Java\n",
    "      JUnit\n",
    "      TestNG\n",
    "      Mockito\n",
    "    C#\n",
    "      NUnit\n",
    "      MSTest\n",
    "      xUnit\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Frameworks are used to discover and run tests defined in certain assembly. After the test run finishes, then the framework provides the statistics about the test run. However in order for test framework to work, you have to write tests in manner in which the testing framework expects.\n",
    "\n",
    ".NET CLI prepacked with with `MSTest`, `NUnit` and `xUnit` testing framework templates. This notebook will focus on `xUnit`, however other options are solid choices as well, and will most likely suit all your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### xUnit\n",
    "\n",
    "#### Creating testing project\n",
    "\n",
    "`dotnet new xunit` will create a new testing project, that will use the `xUnit` project for testing.\n",
    "\n",
    "#### Defining test case\n",
    "\n",
    "`[Fact]` attribute denotes the method as a test method. Such method would test a single condition.\n",
    "\n",
    "```csharp\n",
    "[Fact]\n",
    "public void Addition_2Plus6_Equals8()\n",
    "{\n",
    "    var calculator = new Calculator();\n",
    "    var result = calculator.Calculate(\"2 + 6\");\n",
    "\n",
    "    Assert.Equals(8, result);\n",
    "}\n",
    "```\n",
    "\n",
    "#### Multiple test cases with single method\n",
    "\n",
    "`[Theory]` attribute can be used to run the same test against multiple inputs. In this case test method must also accept arguments, these arguments are passed using `[InlineData]` or similar method.\n",
    "\n",
    "```csharp\n",
    "[Theory]\n",
    "[InlineData(1, 2, 3)]\n",
    "[InlineData(2, 5, 7)]\n",
    "[InlineData(5, 5, 10)]\n",
    "public void Addition_2Numbers_EqualsExpectedResult(int firstNumber, int secondNumber, int expectedResult)\n",
    "{\n",
    "    var calculator = new Calculator();\n",
    "    var result = calculator.Calculate($\"{firstNumber} + ${secondNumber}\");\n",
    "\n",
    "    Assert.Equals(expectedResult, result);\n",
    "}\n",
    "```\n",
    "\n",
    "In this example this one method definition would create 3 different test cases. Arguments in passed via `[InlineData]` must match those that the theory method accepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Test behavior, not implementation\n",
    "\n",
    "Test how the class behaves against it's public interface. Not how it uses it's dependencies or how it manages it's internals.\n",
    "\n",
    "Good:\n",
    "```csharp\n",
    "public void Test_List_CountsCorrectly() {\n",
    "    var list = new List<int>();\n",
    "    list.Add(123);\n",
    "\n",
    "    Assert.Equals(1, list.Count());\n",
    "}\n",
    "```\n",
    "\n",
    "Bad:\n",
    "```csharp\n",
    "public void Test_List_CountsCorrectly() {\n",
    "    var list = new List<int>();\n",
    "    list.Add(123);\n",
    "\n",
    "    var size = GetFieldViaReflection<int>(list, \"_size\");\n",
    "    Assert.Equals(1, size);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For a more elaborate example - consider the following class, which can save a record to the database using `Entity Framework` and also select from the database using it as well:\n",
    "\n",
    "```csharp\n",
    "public class StudentRepository\n",
    "{\n",
    "    private readonly IDbContext _dbContext;\n",
    "\n",
    "    public StudentRepository(IDbContext dbContext)\n",
    "    {\n",
    "        _dbContext = dbContext;\n",
    "    }\n",
    "\n",
    "    public void Save(Student student)\n",
    "    {\n",
    "        _dbContext.Students.Add(student);\n",
    "        await _dbContext.SaveChanges();\n",
    "    }\n",
    "\n",
    "    public List<Student> GetAll()\n",
    "    {\n",
    "        return _dbContext.Students.ToList();\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Bad example** where the underlying state is tested:\n",
    "\n",
    "```csharp\n",
    "[Fact]\n",
    "public void Save_PersistsStudent()\n",
    "{\n",
    "    var fakeDbContext = CreateFakeDbContext();\n",
    "    var studentRepository = new StudentRepository(fakeDbContext);\n",
    "\n",
    "    var id = Guid.NewGuid();\n",
    "    studentRepository.Save(new Student { Id = id });\n",
    "\n",
    "    Assert.True(fakeDbContext.Students.Any(s => s.Id == id));\n",
    "}\n",
    "```\n",
    "\n",
    "This example does white-box testing and it tests the way how the `StudentRepository` class is implemented rather than what it does. If the underlying implementation of `StudentRepository` class was changed to use some other ORM, then this class would break, although from the user's perspective the behavior would remain the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Better example** of how to test such case:\n",
    "\n",
    "```csharp\n",
    "[Fact]\n",
    "public void Save_PersistsStudent()\n",
    "{\n",
    "    var fakeDbContext = CreateFakeDbContext();\n",
    "    var studentRepository = new StudentRepository(fakeDbContext);\n",
    "\n",
    "    var id = Guid.NewGuid();\n",
    "    studentRepository.Save(new Student { Id = id });\n",
    "\n",
    "    var students = studentRepository.GetAll();\n",
    "\n",
    "    Assert.True(students.Any(s => s.Id == id));\n",
    "}\n",
    "```\n",
    "\n",
    "In this case the tests asserts against the output of public interface of the class. It tests the behavior that after saving, the method that returns all students will return the newly saved student as well. This is black-box testing approach, where you ignore the implementation details and only test against the public interface of the class.\n",
    "\n",
    "When tested like this, it allows for the implementation details of the class to change, without needing to update the tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integration testing\n",
    "\n",
    "ASP.NET provides `TestServer` feature, which could be used to develop integration tests against the ASP.NET API.\n",
    "\n",
    "To start off add package to the xUnit project:\n",
    "\n",
    "```\n",
    "dotnet add package Microsoft.AspNetCore.Mvc.Testing\n",
    "```\n",
    "\n",
    "Add class fixture to the test class:\n",
    "\n",
    "```csharp\n",
    ": IClassFixture<WebApplicationFactory<Program>>\n",
    "```\n",
    "\n",
    "`Program` class must be the startup class of the web API that you want to test. If the `Program` class created implicitly, then it will have insufficient accessibility to be reached by other assemblies by default. In such case an additional `public partial` class will have be added to the `Program.cs` file : `public partial class Program { }`.\n",
    "\n",
    "Wire up everything in constructor:\n",
    "\n",
    "```csharp\n",
    "public UnitTest1(WebApplicationFactory<Program> factory)\n",
    "{\n",
    "    _factory = factory;\n",
    "    _client = factory.CreateClient(new WebApplicationFactoryClientOptions\n",
    "    {\n",
    "        AllowAutoRedirect = false\n",
    "    });\n",
    "}\n",
    "```\n",
    "\n",
    "Now inside the test `_client` can be used to make HTTP requests against the running server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Assertions\n",
    "\n",
    "Assertions are used to ensure that the code under test performed what was expected of it. Testing libraries provide various assertion methods that can be used to verify states of the objects. If the assertion fails, then the assertion method usually throws an exception (typically specific to that testing framework)\n",
    "\n",
    "It is considered a good practice to assert on the public contracts of classes and not on their internal implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Test method structure \n",
    "\n",
    "Tests methods tend to follow a certain structure within the test.\n",
    "\n",
    "Most common structure is like this:\n",
    "1. Arrange the classes, data and dependencies for test.\n",
    "2. Act the test - execute the methods under testing.\n",
    "3. Assert that the results is what was expected.\n",
    "\n",
    "Some guidelines suggest that these blocks should be followed within the methods as much possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Test doubles\n",
    "\n",
    "```mermaid\n",
    "mindmap\n",
    "  root((Test doubles))\n",
    "    [**Fakes**\n",
    "\n",
    "        Imitates real work implementation\n",
    "    ]\n",
    "    [**Stubs**\n",
    "\n",
    "        Uses hard coded or otherwise fixed values and/or assertions\n",
    "    ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In order to isolate the code under test from irrelevant dependencies test doubles can be used. Test doubles provide substitute implementation that allows the code under test to work but be more deterministic, or to force the expected scenarios or behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Fakes\n",
    "\n",
    "Fakes are implementations that are compatible with the real world implementations, but uses are more isolated approach that is easier to manager when testing. An example would be for repository, which in real world cases would be using database for persistance, could be substituted with the fake implementation that persists everything in memory.\n",
    "\n",
    "Such fake implementation could not be used for real world scenarios, but depending on what is being tested and fidelity of the fake implementation, it could be sufficient to test the desired use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Stubs\n",
    "\n",
    "Stubs (might also be called *mocking*) provide predefined \"stubbed\" implementations for dependencies. For example with the same repository which in real cases uses database for persistence, the stubbed repository could simply return `Task.CompletedTask` from save method, and return a single hardcoded value when trying to get all records.\n",
    "\n",
    "Stubs can also be set up to verify certain conditions, such as that `Save` method was called exactly once on the stubbed class. However this should be done with caution, as using this incorrectly could lead to testing the way class was implemented and not what it does.\n",
    "\n",
    "In .NET packages like `Moq` could be used to help with this. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "name": "polyglot-notebook"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "languageName": "csharp",
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
